{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color, data, io, exposure\n",
    "from skimage.feature import canny, hog\n",
    "import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import image\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xlwings as xw\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "from distutils import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function that takes parameters of the project code.\n",
    "def handle_args():\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('-f')\n",
    "    parser.add_argument('--image_name', default='p001')\n",
    "    parser.add_argument('--max_depth', default=20)\n",
    "    parser.add_argument('--avg_patch_size', default=39)\n",
    "    parser.add_argument('--sum_patch_size', default=8)\n",
    "    parser.add_argument('--sd_patch_size', default=16)\n",
    "    parser.add_argument('--hog_patch_size', default=1)\n",
    "    parser.add_argument('--neigh_patch_size', default=1)\n",
    "    parser.add_argument('--use_distance_features', default=True)\n",
    "    parser.add_argument('--use_sd_features', default=False)\n",
    "    parser.add_argument('--use_sum_features', default=True)\n",
    "    parser.add_argument('--use_avg_features', default=True)\n",
    "    parser.add_argument('--use_edge_features', default=False)\n",
    "    parser.add_argument('--use_haris_corner', default=True)\n",
    "    parser.add_argument('--use_hog_features', default=False)\n",
    "    parser.add_argument('--use_fast_feature', default=True)\n",
    "    parser.add_argument('--use_orb_feature', default=False)\n",
    "    parser.add_argument('--use_neighborhood_features', default=False)\n",
    "    parser.add_argument('--save_to_file', default=False)\n",
    "    args=parser.parse_args()\n",
    "    args.use_fast_feature = util.strtobool(str(args.use_fast_feature))\n",
    "    args.use_orb_feature = util.strtobool(str(args.use_orb_feature))\n",
    "    args.use_neighborhood_features = util.strtobool(str(args.use_neighborhood_features))\n",
    "    args.use_hog_features = util.strtobool(str(args.use_hog_features))\n",
    "    args.use_haris_corner = util.strtobool(str(args.use_haris_corner))\n",
    "    args.use_edge_features = util.strtobool(str(args.use_edge_features))\n",
    "    args.use_avg_features = util.strtobool(str(args.use_avg_features))\n",
    "    args.use_sum_features = util.strtobool(str(args.use_sum_features))\n",
    "    args.use_sd_features = util.strtobool(str(args.use_sd_features))\n",
    "    args.use_distance_features = util.strtobool(str(args.use_distance_features))\n",
    "    args.save_to_file = util.strtobool(str(args.save_to_file))\n",
    "\n",
    "    args.avg_patch_size = int(args.avg_patch_size)\n",
    "    args.sum_patch_size = int(args.sum_patch_size)\n",
    "    args.sd_patch_size = int(args.sd_patch_size)\n",
    "    args.hog_patch_size = int(args.hog_patch_size)\n",
    "    args.neigh_patch_size = int(args.neigh_patch_size)\n",
    "    args.max_depth = int(args.max_depth)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function shows the images which is passed as parameters with a special format\n",
    "def show_image(dic, image_name, save = False):\n",
    "    fig, axes = plt.subplots(1, len(dic), figsize = (24,16))\n",
    "    ax = axes.ravel()\n",
    "    \n",
    "    for i in dic:\n",
    "        ax[list(dic.keys()).index(i)].set_title(i)\n",
    "        ax[list(dic.keys()).index(i)].imshow(dic[i], cmap = \"gray\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    time = datetime.datetime.now()\n",
    "    plt.savefig(f'results/{image_name}.{time.hour}.{time.minute}.png', format=\"png\")\n",
    "    if not save:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the last row in excel\n",
    "def find_last_row_in_excel(workbooklocation,sheetname,columnletter): \n",
    "    wb = xw.Book(workbooklocation)\n",
    "    X = wb.sheets[sheetname].range(columnletter + str(wb.sheets[sheetname].cells.last_cell.row)).end('up').row + 1\n",
    "    cell = columnletter + str(X)\n",
    "    print(cell)\n",
    "    return cell\n",
    "\n",
    "\n",
    "\n",
    "# This function writes data which is passed as parameters into excel file.\n",
    "def write_data_to_excel(features, max_depth, avg_patch_size, sum_patch_size, mean_abs_err, image_name, finish_time):\n",
    "    try:\n",
    "        wb = xw.Book('mae.xlsx')\n",
    "        sht = wb.sheets['Sheet1']\n",
    "\n",
    "        if(image_name == \"p001\"):\n",
    "            last_row = find_last_row_in_excel('mae.xlsx','Sheet1','A')\n",
    "            sht.range(last_row).value = ['Mean Absolute Error','Image Name', 'Process_time (second)', f'{features}',f'Max Depth: {max_depth}', f'Avg PS: {avg_patch_size}', f'Sum PS: {sum_patch_size}']\n",
    "\n",
    "        last_row = find_last_row_in_excel('MAE.xlsx','Sheet1','A')\n",
    "        sht.range(last_row).value = [f'{float(mean_abs_err)}',f'{image_name}',f'{finish_time}']\n",
    "    except e:\n",
    "        print(\"The file named mae.xlsx is not found in the project directory.\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets the source image\n",
    "def get_source(image_name):\n",
    "    return io.imread(f'Dataset\\\\{image_name}, a_source.png')\n",
    "\n",
    "# This function gets the source image with openCV\n",
    "def get_source_opencv(image_name):\n",
    "    return cv2.imread(f'Dataset\\\\{image_name}, a_source.png')\n",
    "\n",
    "# This function gets the target image with openCV\n",
    "def get_target_opencv(image_name):\n",
    "    return cv2.imread(f'Dataset\\\\{image_name}, b_target.png')\n",
    "\n",
    "# This function gets the target image\n",
    "def get_target(image_name):\n",
    "    return io.imread(f'Dataset\\\\{image_name}, b_target.png')\n",
    "\n",
    "# This function gets the groundtruth image\n",
    "def get_groundtruth(image_name):\n",
    "    return io.imread(f'Dataset\\\\{image_name}, c_groundtruth.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reconstructs an image according to rgb and dimension values which is passed as parameter\n",
    "def reconstruct(r, g, b, dimensions):\n",
    "    image = np.zeros((dimensions[0] * dimensions[1], 3))\n",
    "    for i in range(dimensions[0] * dimensions[1]):\n",
    "        image[i][0] = int(r[i])\n",
    "        image[i][1] = int(g[i])\n",
    "        image[i][2] = int(b[i])\n",
    "    image = np.reshape(image, (dimensions[0], dimensions[1], 3))\n",
    "    return image.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts the image into one dimension\n",
    "def get_one_dim(image, colored = False):\n",
    "    if colored:\n",
    "        return np.reshape(image, (image.shape[0] * image.shape[1], 1, 3))\n",
    "    else:\n",
    "        return np.reshape(image, (image.shape[0] * image.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts the image values which is integer into float type\n",
    "def convert_float(image):\n",
    "    squarer = lambda t: t / 255\n",
    "    vfunc = np.vectorize(squarer)\n",
    "    return vfunc(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates average of special pixel and their neighboors.\n",
    "def get_avg(data, point, patch_size):\n",
    "    first, second, third, fourth = get_quarters(data, point, patch_size)\n",
    "    summation = (np.sum(first) + np.sum(second) + np.sum(third) + np.sum(fourth) + np.sum(data[point[0], point[1]]))\n",
    "    length = (first.shape[0] * first.shape[1] + second.shape[0] * second.shape[1] + \n",
    "             third.shape[0] * third.shape[1] + fourth.shape[0] * fourth.shape[1] + 1)\n",
    "    return summation/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the sum of special pixel and their neigboors.\n",
    "def get_sum(data, point, patch_size):\n",
    "    first, second, third, fourth = get_quarters(data, point, patch_size)\n",
    "    summation = (np.sum(first) + np.sum(second) + np.sum(third) + np.sum(fourth) + np.sum(data[point[0], point[1]]))\n",
    "    return summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the standard deviation of special pixel and their neigboors\n",
    "def get_sd(data, point, patch_size):\n",
    "    first, second, third, fourth = get_quarters(data, point, patch_size)\n",
    "    summation = (np.sum(first) + np.sum(second) + np.sum(third) + np.sum(fourth) + np.sum(data[point[0], point[1]]))\n",
    "    length = (first.shape[0] * first.shape[1] + second.shape[0] * second.shape[1] + \n",
    "             third.shape[0] * third.shape[1] + fourth.shape[0] * fourth.shape[1] + 1)\n",
    "    avg = summation/length\n",
    "    _sum = 0.0\n",
    "    first = first.flatten()\n",
    "    for i in first.flatten():\n",
    "        _sum = _sum + pow((i - avg), 2)\n",
    "    for i in second.flatten():\n",
    "        _sum = _sum + pow((i - avg), 2)\n",
    "    for i in third.flatten():\n",
    "        _sum = _sum + pow((i - avg), 2)\n",
    "    for i in fourth.flatten():\n",
    "        _sum = _sum + pow((i - avg), 2)\n",
    "    result = float(_sum/length)\n",
    "    return math.sqrt(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the neighboorhood of special pixel according to patch_size parameter which define window size like 3x3, 5x5.  \n",
    "# Then returns the quarters of this special pixel.\n",
    "def get_quarters(data, point, patch_size):\n",
    "    #first quarter\n",
    "    x_start = point[0] - patch_size\n",
    "    x_end = point[0] - 1\n",
    "    y_start = point[1]\n",
    "    y_end = point[1] + patch_size\n",
    "    \n",
    "    if x_start < 0:\n",
    "        x_start = 0\n",
    "    if y_end > data.shape[1]:\n",
    "        y_end = data.shape[1]\n",
    "    first = get_portion(data, x_start, x_end, y_start, y_end)\n",
    "    \n",
    "    #second quarter\n",
    "    x_start = point[0] - patch_size\n",
    "    x_end = point[0]\n",
    "    y_start = point[1] - patch_size\n",
    "    y_end = point[1] - 1\n",
    "    \n",
    "    if x_start < 0:\n",
    "        x_start = 0\n",
    "    if y_start < 0:\n",
    "        y_start = 0\n",
    "    second = get_portion(data, x_start, x_end, y_start, y_end)\n",
    "    \n",
    "    #third quarter\n",
    "    x_start = point[0] + 1\n",
    "    x_end = point[0] + patch_size\n",
    "    y_start = point[1] - patch_size\n",
    "    y_end = point[1]\n",
    "    \n",
    "    if x_end > data.shape[0]:\n",
    "        x_end = data.shape[0]\n",
    "    if y_start < 0:\n",
    "        y_start = 0\n",
    "    third = get_portion(data, x_start, x_end, y_start, y_end)\n",
    "    \n",
    "    #fourth quarter\n",
    "    x_start = point[0]\n",
    "    x_end = point[0] + patch_size\n",
    "    y_start = point[1] + 1\n",
    "    y_end = point[1] + patch_size\n",
    "    \n",
    "    if x_end > data.shape[0]:\n",
    "        x_end = data.shape[0]\n",
    "    if y_end > data.shape[1]:\n",
    "        y_end = data.shape[1]\n",
    "    fourth = get_portion(data, x_start, x_end, y_start, y_end)\n",
    "    \n",
    "    return first, second, third, fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the certain parts of data according to given parameters\n",
    "def get_portion(data, x_start, x_end, y_start, y_end):\n",
    "    return data[x_start:x_end + 1, y_start:y_end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a Random Forest Regressior, Then, doing fitting process.\n",
    "def fit_channel(X, y, n_estimator = 16):\n",
    "    classifier = RandomForestRegressor(n_estimators=n_estimator)\n",
    "    classifier.fit(X, y)\n",
    "    return classifier\n",
    "\n",
    "# This function creates a Random Forest Regressior with Randomized Search Cross Validation to find best model parameters.\n",
    "def model_with_best_parameters(X,y):\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 50, num = 5)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    \n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestRegressor()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X, y)\n",
    "    print(rf_random.best_params_)\n",
    "    \n",
    "    return rf_random.best_estimator_\n",
    "\n",
    "# This function does predict process according to given classifier.\n",
    "def predict_channel(classifier, X):\n",
    "    # The score method returns the accuracy of the model\n",
    "    return classifier.predict(X)\n",
    "\n",
    "# This function finds edges on image with canny\n",
    "def get_edge_feature(image):\n",
    "    edges = canny(image)\n",
    "    return np.reshape(edges, (image.shape[0] * image.shape[1], 1))\n",
    "\n",
    "# This function finds sum value of each pixels of their neighbor pixels\n",
    "def get_sum_feature(data, patch_size):\n",
    "    sum_feature = np.zeros((data.shape[0], data.shape[1]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            sum_feature[i][j] = get_sum(data, (i, j), patch_size)\n",
    "    return np.reshape(sum_feature, (sum_feature.shape[0] * sum_feature.shape[1], 1))\n",
    "\n",
    "# This function finds average value of each pixels of their neighbor pixels\n",
    "def get_avg_feature(data, patch_size):\n",
    "    avg_feature = np.zeros((data.shape[0], data.shape[1]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            avg_feature[i][j] = get_avg(data, (i, j), patch_size)\n",
    "    return np.reshape(avg_feature, (avg_feature.shape[0] * avg_feature.shape[1], 1))\n",
    "\n",
    "# This function finds neighborhood values of each pixels of their neighbor pixels\n",
    "def get_neighborhood_feature(data, patch_size):\n",
    "    neigh = np.empty((1,int((patch_size * 2 + 1)**2 - 1)))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            feature = get_neighborhood(data, (i, j), patch_size)\n",
    "            neigh = np.vstack((neigh,feature))\n",
    "    neigh_deleted = np.delete(neigh, 0, 0)\n",
    "    return neigh_deleted\n",
    "\n",
    "# This function finds standard deviation of each pixels of their neighbor pixels\n",
    "def get_sd_feature(data, patch_size):\n",
    "    sd_feature = np.zeros((data.shape[0], data.shape[1]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            sd_feature[i][j] = get_sd(data, (i, j), patch_size)\n",
    "    return np.reshape(sd_feature, (sd_feature.shape[0] * sd_feature.shape[1], 1))\n",
    "\n",
    "# This function finds Histogram of Oriented Gradients (HOG) value of each pixels\n",
    "def get_hog_feature(data, hog_patch_size):\n",
    "    fd, hog_image = hog(data, orientations=8, pixels_per_cell=(2, 2),\n",
    "                    cells_per_block=(2, 2), visualize=True, multichannel=False)\n",
    "\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "    return get_sum_feature(hog_image_rescaled, hog_patch_size)\n",
    "\n",
    "# This function finds corners value of each pixels with using Haris Corner Detection\n",
    "def get_haris_corner(image,img):\n",
    "    gray_img = np.float32(image)\n",
    "    dst = cv2.cornerHarris(gray_img, blockSize=2, ksize=3, k=0.04)\n",
    "    dst = cv2.dilate(dst, None)\n",
    "    return np.reshape(dst, (dst.shape[0]*dst.shape[1],1))\n",
    "\n",
    "# This function finds corners value of each pixels with using FAST Corner Detection Algorithm\n",
    "def get_fast_feature(gray_img, img):\n",
    "    fast = cv2.FastFeatureDetector_create()\n",
    "    fast.setNonmaxSuppression(False)\n",
    "\n",
    "    kp = fast.detect(gray_img, None)\n",
    "    kp_img = cv2.drawKeypoints(img, kp, None, color=(0, 255, 0))\n",
    "    \n",
    "    return np.reshape(kp_img, (kp_img.shape[0] * kp_img.shape[1], 3))\n",
    "\n",
    "# This function finds key points values of each pixels with using ORB (Oriented FAST and Rotated Brief) algorithm\n",
    "def get_orb_feature(gray_img, img):\n",
    "    orb = cv2.ORB_create(nfeatures=2000)\n",
    "    kp, des = orb.detectAndCompute(gray_img, None)\n",
    "\n",
    "    kp_img = cv2.drawKeypoints(img, kp, None, color=(0, 255, 0), flags=0)\n",
    "    \n",
    "    return np.reshape(kp_img, (kp_img.shape[0] * kp_img.shape[1], 3))\n",
    "\n",
    "# This function finds neighborhood values of special pixels of their neighbor pixels\n",
    "def get_neighborhood(data, point, patch_size):\n",
    "    quarter_len = int(((patch_size * 2 + 1)**2 - 1) / 4)\n",
    "    first, second, third, fourth = get_quarters(data, point, patch_size)\n",
    "    \n",
    "    first = first.flatten()\n",
    "    second = second.flatten()\n",
    "    third = third.flatten()\n",
    "    fourth = fourth.flatten()\n",
    "    \n",
    "    feature_mean = np.concatenate((first, second,third,fourth), axis=None)\n",
    "    feature_mean = np.mean(feature_mean)\n",
    "    if(len(first) < int(quarter_len)):\n",
    "        array_first_mean = np.empty((quarter_len - len(first),)) \n",
    "        array_first_mean[:] = feature_mean\n",
    "\n",
    "        first = np.concatenate((first,array_first_mean), axis=None)\n",
    "        \n",
    "    if(len(second) < int(quarter_len)):\n",
    "        array_second_mean = np.empty((quarter_len - len(second),)) \n",
    "        array_second_mean[:] = feature_mean\n",
    "\n",
    "        second = np.concatenate((second,array_second_mean), axis=None)\n",
    "       \n",
    "    if(len(third) < int(quarter_len)):\n",
    "        array_third_mean = np.empty((quarter_len - len(third),)) \n",
    "        array_third_mean[:] = feature_mean\n",
    "\n",
    "        third = np.concatenate((third,array_third_mean), axis=None)\n",
    "    \n",
    "    if(len(fourth) < int(quarter_len)):\n",
    "        array_fourth_mean = np.empty((quarter_len - len(fourth),)) \n",
    "        array_fourth_mean[:] = feature_mean\n",
    "        \n",
    "        fourth = np.concatenate((fourth,array_fourth_mean), axis=None)\n",
    "       \n",
    "    features = np.concatenate((first, second,third,fourth), axis=None)\n",
    "    return features\n",
    "\n",
    "# This function is helper function for doing called function as generic \n",
    "def get_feature(func, image_name, X, X_test):\n",
    "    source_opencv = get_source_opencv(image_name)\n",
    "    source_gray_2d_opencv = cv2.cvtColor(source_opencv, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    feature_x = func(source_gray_2d_opencv, source_opencv)\n",
    "    result_x = np.hstack((X,feature_x))\n",
    "    \n",
    "    target_opencv = get_target_opencv(image_name)\n",
    "    target_opencv = cv2.cvtColor(target_opencv, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    feature_test = func(target_opencv,target_opencv)\n",
    "    result_test = np.hstack((X_test, feature_test))\n",
    "    \n",
    "    return result_x, result_test\n",
    "\n",
    "# This function finds each pixels distance to top,bottom,right and left distance\n",
    "def get_distance_feature(data):\n",
    "    distance_down_feature = np.zeros((data.shape[0], data.shape[1]))\n",
    "    distance_up_feature = np.zeros((data.shape[0], data.shape[1]))\n",
    "    distance_left_feature = np.zeros((data.shape[0], data.shape[1]))\n",
    "    distance_right_feature = np.zeros((data.shape[0], data.shape[1]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            distance_up_feature[i][j] = i\n",
    "            distance_down_feature[i][j] = data.shape[0] - i\n",
    "            distance_left_feature[i][j] = j\n",
    "            distance_right_feature[i][j] = data.shape[1] - j\n",
    "    return np.reshape(distance_up_feature, (data.shape[0] * data.shape[1], 1)), np.reshape(distance_down_feature, (data.shape[0] * data.shape[1], 1)), np.reshape(distance_left_feature, (data.shape[0] * data.shape[1], 1)), np.reshape(distance_right_feature, (data.shape[0] * data.shape[1], 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
